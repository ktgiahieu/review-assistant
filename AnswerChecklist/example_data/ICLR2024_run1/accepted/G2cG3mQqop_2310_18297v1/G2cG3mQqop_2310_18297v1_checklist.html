<!DOCTYPE html>
<html lang='en'>
<head>
<meta charset='UTF-8'>
<meta name='viewport' content='width=device-width, initial-scale=1.0'>
<title>Checklist Review: Image Clustering Conditioned on Text Criteria (IC|TC)</title>
<style>
            body { font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #333; max-width: 900px; margin: 20px auto; padding: 0 15px; }
            h1, h2 { color: #111; border-bottom: 2px solid #eee; padding-bottom: 10px; }
            h1 { font-size: 2em; }
            h2 { font-size: 1.5em; margin-top: 40px; }
            table { width: 100%; border-collapse: collapse; margin-top: 20px; }
            th, td { padding: 12px 15px; text-align: left; border-bottom: 1px solid #ddd; }
            th { background-color: #f8f8f8; font-weight: 600; }
            td:nth-child(1) { width: 5%; }
            td:nth-child(3) { width: 10%; text-align: center; font-weight: bold; }
            .abstract-section { margin-left: 20px; }
        </style>
</head>
<body>
<h1>Image Clustering Conditioned on Text Criteria (IC|TC)</h1>
<p><strong>Paper Link:</strong> <a href='https://openreview.net/forum?id=G2cG3mQqop'>https://openreview.net/forum?id=G2cG3mQqop</a></p>
<h2>Formatted Abstract</h2>
<div class='abstract-section'><p><strong>Background and Objectives:</strong> Classical image-clustering techniques cannot be steered to match a user’s desired grouping principle. The authors propose a new paradigm in which the user supplies an arbitrary natural-language criterion to control the clustering outcome.</p>
<p><strong>Material and Methods:</strong> IC|TC uses a vision-language model (e.g., LLaVA) to caption each image according to the supplied text criterion, then a large language model (GPT-4 or variants) (i) proposes raw labels, (ii) merges them into K cluster names, and (iii) assigns every image to one of the clusters; the user may iteratively refine the criterion.</p>
<p><strong>Results:</strong> Across six datasets (CIFAR-10/100, STL-10, Stanford-40, PPMI, and a synthetic CIFAR-10-Gen) IC|TC achieves 0.91–0.98 ACC on object-based clustering and consistently outperforms strong deep-clustering baselines such as SCAN. Experiments show controllability over different criteria (action, location, mood) and cluster granularities as well as a prompt-based fairness mitigation case study.</p>
<p><strong>Conclusion:</strong> Leveraging instruction-tuned foundation models enables practical, controllable image clustering with minimal human effort; the framework opens avenues for language-directed computer-vision pipelines and highlights the importance of prompt engineering and fairness considerations.</p></div>
<h2>A. Compliance</h2>
<table>
<tr><th>ID</th><th>Question</th><th>Answer</th><th>Reasoning</th></tr>
<tr>
<td>A1</td>
<td>Does the paper discuss potential positive societal impacts of the work?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>An explicit Ethics Statement (p. 8, “Ethics Statement”) argues that user-controllable clustering can aid practical data organization and fairness auditing.</td>
</tr>
<tr>
<td>A2</td>
<td>Does the paper discuss potential negative societal impacts or risks of the work?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>The same Ethics Statement warns about bias propagation from foundation models and malicious use for discriminatory clustering.</td>
</tr>
<tr>
<td>A3</td>
<td>If negative societal impacts or risks are identified, does the paper discuss potential mitigation strategies?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Section 4.4 shows a prompt-based mitigation experiment adding “Do not consider gender” to reduce gender imbalance.</td>
</tr>
<tr>
<td>A5</td>
<td>Are the creators or original owners of all used assets (e.g., code, data, models) properly credited in the paper?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>The paper cites LLaVA, GPT-4, Blip-2, datasets such as CIFAR and Stanford-40, and provides full references.</td>
</tr>
<tr>
<td>A6</td>
<td>Are the license and terms of use for all used assets explicitly mentioned and respected in the paper?</td>
<td style="background-color: #ffe6e6; color: #a30000;">No</td>
<td>While models/datasets are cited, the manuscript does not state their specific licenses (e.g., MIT, CC-BY) or usage terms.</td>
</tr>
<tr>
<td>A17</td>
<td>Does the research avoid primarily aiming to develop or enhance the lethality of weapons systems?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>The work is about image clustering for data analysis; no military or weapons applications are suggested.</td>
</tr>
<tr>
<td>A26</td>
<td>Have potential biases in the data, algorithms, models, or interpretation of results been carefully considered and transparently discussed?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Bias propagation is discussed in §4.4 with a quantitative fairness experiment on the FACET benchmark.</td>
</tr>
<tr>
<td>A31</td>
<td>Does the paper accurately and honestly report all findings, including any limitations or negative results, without fabrication, falsification, or selective misrepresentation of data or outcomes?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Results include ablations where performance drops (e.g., ClipCap baseline, GPT-3.5 vs GPT-4) and a discussion of token-limit failures.</td>
</tr>
</table>
<h2>B. Contribution</h2>
<table>
<tr><th>ID</th><th>Question</th><th>Answer</th><th>Reasoning</th></tr>
<tr>
<td>B1</td>
<td>Are the main claims clearly stated in the abstract?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>The abstract declares the new methodology, user control, and superior empirical performance.</td>
</tr>
<tr>
<td>B6</td>
<td>Are the specific contributions clearly summarized?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Section ‘Contribution’ enumerates task definition and the IC|TC method plus empirical validation.</td>
</tr>
<tr>
<td>B7</td>
<td>Is the novelty of the contributions clearly articulated relative to prior work?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Authors contrast their paradigm to classical clustering, zero-shot classification, and deep clustering literature (§2).</td>
</tr>
<tr>
<td>B13</td>
<td>Does the paper explicitly discuss the limitations of the work performed?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Limitations include reliance on proprietary GPT-4 API (§Reproducibility) and model bias (§4.4).</td>
</tr>
<tr>
<td>B18</td>
<td>Is the computational efficiency and scalability of the proposed method discussed?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>API cost (<$3 k), token-length issues, and optimizations with dictionaries are reported (§Reproducibility, §8.3).</td>
</tr>
</table>
<h2>C. Soundness</h2>
<table>
<tr><th>ID</th><th>Question</th><th>Answer</th><th>Reasoning</th></tr>
<tr>
<td>C10</td>
<td>Is the experimental methodology clearly described?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Datasets, prompts, evaluation metrics (ACC, NMI, ARI), and ablation protocols are detailed in §4 and Appendix 8.</td>
</tr>
<tr>
<td>C19</td>
<td>Are evaluation metrics clearly defined?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>ACC, NMI, ARI, and fairness disparity percentages are defined or referenced with standard meaning.</td>
</tr>
<tr>
<td>C34</td>
<td>Are ablation studies conducted to isolate the impact of different components?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Separate ablations for VLM choice, LLM size, and removal of steps demonstrate component necessity (§4.5, Appendices).</td>
</tr>
<tr>
<td>C41</td>
<td>Does the paper disclose all resources (code, data, scripts) necessary to reproduce the main experimental results?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>A public GitHub link is provided and prompts/dataset splits are in the appendix.</td>
</tr>
<tr>
<td>C42</td>
<td>Will the source code be made publicly available under a suitable license?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Code is already released at the stated GitHub repository.</td>
</tr>
<tr>
<td>C1</td>
<td>Are theorems, lemmas, or key theoretical results clearly stated?</td>
<td style="background-color: #f0f0f0; color: #555;">NA</td>
<td>The paper is empirical and does not present formal theorems.</td>
</tr>
</table>
<h2>D. Presentation</h2>
<table>
<tr><th>ID</th><th>Question</th><th>Answer</th><th>Reasoning</th></tr>
<tr>
<td>D1</td>
<td>Is the abstract a clear and concise summary of the paper’s content?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>It introduces motivation, method, and headline results within the length limit.</td>
</tr>
<tr>
<td>D5</td>
<td>Is the paper organized in a logical and easy-to-follow manner?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Sections progress from motivation to method, experiments, analysis, related work, conclusion.</td>
</tr>
<tr>
<td>D17</td>
<td>Are all figures and tables essential and well-integrated into the text?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Figures illustrate pipeline and qualitative results; tables summarize quantitative metrics and ablations with references in text.</td>
</tr>
<tr>
<td>D24</td>
<td>Are all figures and tables correctly referenced and discussed in the text?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>Each figure/table has a numbered call-out (e.g., "Figure 1") in the narrative.</td>
</tr>
<tr>
<td>D28</td>
<td>Does the paper adhere to standard formatting guidelines (e.g., margins, font size, length)?</td>
<td style="background-color: #e6ffed; color: #00611d;">Yes</td>
<td>The manuscript appears to follow OpenReview/ICLR template without obvious violations.</td>
</tr>
</table>
</body>
</html>