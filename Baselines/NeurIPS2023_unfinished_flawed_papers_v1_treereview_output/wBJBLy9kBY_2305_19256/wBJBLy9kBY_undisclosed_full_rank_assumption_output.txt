{
  "full_review": "**Summary:**\nThis paper introduces Ambient Diffusion, a novel diffusion-based framework designed to learn clean data distributions from highly corrupted samples. The key idea is to inject additional measurement distortion during the diffusion process and train the model to predict the original corrupted image from a further corrupted version. The authors provide theoretical guarantees, demonstrating that the model recovers the conditional expectation of the uncorrupted image under a mild randomness condition. They showcase the effectiveness of their method on CelebA, CIFAR-10, and AFHQ datasets, even with high levels of corruption, and demonstrate its application in fine-tuning foundation models while mitigating memorization.\n\n**Strengths:**\nThe paper presents a theoretically grounded and empirically validated approach to training diffusion models with corrupted data. The \"mild randomness condition\" offers a practical and easily satisfied requirement for the method's success. The empirical results demonstrate state-of-the-art performance in image restoration tasks compared to existing diffusion models, even when trained solely on corrupted data. The application to fine-tuning foundation models and the observed reduction in memorization are significant and valuable contributions.\n\n**Weaknesses:**\nThe paper lacks a comprehensive analysis of corruptions that violate the \"mild randomness condition.\" There is no direct quantitative analysis of the computational overhead introduced by the additional measurement distortion. The analysis lacks granularity regarding the sensitivity of specific architectural components and hyperparameters to performance degradation under varying corruption levels. The paper does not explicitly analyze whether the type of corruption disproportionately affects certain features or subgroups within the dataset, nor does it employ specific bias detection/mitigation techniques. The justification for using FID as the primary evaluation metric is lacking, and the paper does not report statistical significance measures.\n\n**Questions:**\n1.  Could the authors provide specific examples of corruption types that would violate the \"mild randomness condition\" and discuss potential strategies to address such scenarios?\n2.  Can the authors provide a more detailed per-step cost analysis of the \"additional measurement distortion\" and discuss its impact on scalability and efficiency?\n3.  Could the authors elaborate on the optimization process for hyperparameters under different corruption scenarios and provide more detailed guidelines for selecting appropriate values?\n4.  How does the proposed method perform on datasets with severe class imbalance or under-representation of certain features, and what strategies can be employed to mitigate potential biases?\n5.  What is the justification for using FID as the primary evaluation metric, and can the authors provide additional metrics or statistical significance measures to support their claims?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n7 accept, but needs minor improvements\n\n**Confidence:**\n4 You are confident in your assessment, but not absolutely certain.",
  "feedback_comments": [
    "The paper claims a 'one-size-fits-all' guarantee due to the mild randomness condition, but it lacks a comprehensive discussion of scenarios where this condition might not hold, especially under structured noise or adversarial attacks. This overstates the generality of the method (Section: Sampling, Paragraph: 12, 17).",
    "The paper needs to provide a more detailed quantitative analysis of the computational overhead introduced by the 'additional measurement distortion' at each diffusion step. The absence of a precise per-step cost analysis makes it difficult to assess the method's scalability and efficiency (Section: Reduction > Finetuning Deepfloyd’s IF., Paragraph: 2).",
    "The paper lacks granularity regarding the sensitivity of specific architectural components and hyperparameters to performance degradation under varying corruption levels. The hyperparameter guidelines are superficial and lack a detailed explanation of how these values were optimized for different corruption scenarios (Section: Reduction > Finetuning Deepfloyd’s IF., Paragraph: 3).",
    "The justification for using FID as the primary evaluation metric is lacking, and the paper does not report statistical significance measures. This makes it difficult to assess the robustness and reliability of the reported improvements (Section: Sampling, Paragraph: 5, Section: Training from scratch on corrupted data, Paragraph: 9).",
    "The paper needs to provide a more comprehensive discussion of the limitations in handling datasets with severe class imbalance or under-representation of certain features. The method reduces memorization of under-represented groups, but this comes at the cost of decreased performance (Section: Finetuning foundation models on corrupted data > Memorization., Paragraph: 3).",
    "The paper lacks a comprehensive theoretical analysis of computational complexity in terms of FLOPs or other relevant metrics. The paper also falls short in providing a detailed breakdown of inference time. (Section: Reduction > Finetuning Deepfloyd’s IF., Paragraph: 5)",
    "The absence of a practical algorithm to implement the theoretical reconstruction raises concerns about the accessibility and completeness of the proofs. The paper should provide more guidance on how to choose the level of distortion to balance robustness and fidelity.",
    "The paper lacks direct comparisons to other generative baselines trained on corrupted data for CelebA-HQ and AFHQ datasets. This makes it difficult to assess the relative performance of the proposed method compared to existing techniques (Section: Training from scratch on corrupted data, Paragraph: 9)."
  ]
}