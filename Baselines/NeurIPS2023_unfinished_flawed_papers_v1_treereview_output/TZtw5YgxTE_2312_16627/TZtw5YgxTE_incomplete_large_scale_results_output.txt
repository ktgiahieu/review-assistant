{
  "full_review": "**Summary:**\nThis paper introduces Mutual Information Maximization for Dataset Distillation (MIM4DD), a novel approach to dataset distillation that explicitly maximizes the mutual information (MI) between real and synthetic data within a contrastive learning framework. The method aims to synthesize a compact dataset that retains the performance of training on the full dataset. MIM4DD pulls together positive pairs (same class) and repels negative pairs (different classes) in a learned embedding space, producing informative synthetic samples. The authors demonstrate through experiments on MNIST, SVHN, CIFAR-10, and CIFAR-100 that MIM4DD can match or surpass state-of-the-art methods with a small number of distilled images per class and generalizes across architectures.\n\n**Strengths:**\nThe paper's strengths lie in its novel formulation of dataset distillation as a mutual information maximization problem, the derivation of a numerically feasible lower bound via contrastive learning, and the empirical results demonstrating improved performance over existing methods. The plug-and-play nature of MIM4DD, allowing it to augment existing DD pipelines, is also a notable strength. The theoretical grounding in information theory provides a solid foundation for the approach.\n\n**Weaknesses:**\nThe paper has several weaknesses. It lacks a thorough analysis of the computational complexity of MIM4DD compared to existing methods. The ablation studies are insufficient to fully demonstrate the individual contributions of the different components of the MIM4DD framework, particularly due to the absence of Appendix C. The paper inadequately addresses the potential biases and limitations in the datasets used for evaluation, raising concerns about the generalizability of its findings. The limitations of the proposed MIM4DD approach are not clearly articulated or adequately addressed in the discussion. The comparison against alternative dataset distillation methods is not sufficiently justified.\n\n**Questions:**\n1.  Could the authors provide a more detailed analysis of the computational complexity of MIM4DD, including a comparison of training time and memory usage with other dataset distillation methods on the benchmark datasets?\n2.  Can the authors include the content of Appendix C, or provide more detailed ablation studies in the main text, to better demonstrate the individual contributions of the mutual information maximization component and the contrastive learning framework?\n3.  How does the performance of MIM4DD vary with different network architectures beyond the three-layer ConvNet?\n4.  Could the authors discuss the potential biases in the datasets used (MNIST, SVHN, CIFAR-10, CIFAR-100) and how these biases might affect the generalizability of MIM4DD?\n5.  How sensitive is MIM4DD to the choice of hyperparameters, such as the temperature parameter τ and the weighting factor λ?\n6.  Can the authors provide a more detailed justification for the choice of baseline methods and discuss why certain alternative dataset distillation methods were excluded from the comparison?\n7.  How does MIM4DD perform on datasets with noisy labels or imbalanced class distributions?\n\n**Soundness:**\n3 good\n\n**Presentation:**\n3 good\n\n**Contribution:**\n3 good\n\n**Rating:**\n6 marginally above the acceptance threshold\n\n**Confidence:**\n3 You are fairly confident in your assessment.",
  "feedback_comments": [
    "The paper does not explicitly identify potential failure cases or scenarios where MIM4DD might underperform. The discussion section lacks a thorough analysis of trade-offs, such as computational cost and hyperparameter sensitivity. It's unclear whether the limitations are specific to MIM4DD or apply to other dataset distillation approaches due to the absence of a comparative analysis of limitations. The paper does not propose potential avenues for future research to mitigate the identified limitations of MIM4DD. There is no detailed analysis of the computational costs associated with MIM4DD compared to other methods. Finally, the paper does not address the performance of MIM4DD under challenging conditions like noisy labels or imbalanced classes.",
    "The paper inadequately addresses the potential biases and limitations in the datasets (MNIST, SVHN, CIFAR-10, and CIFAR-100) used for evaluating the MIM4DD method, raising concerns about the generalizability of its findings. The paper fails to adequately address the potential biases present in the MNIST, SVHN, CIFAR-10, and CIFAR-100 datasets and their potential influence on the performance of MIM4DD. While the authors claim the datasets \"span grayscale and color imagery as well as 10- and 100-way classification,\" they fail to explicitly address potential biases within these datasets, the limitations of relying solely on image classification datasets for dataset distillation, and the potential for overfitting to the specific characteristics of these datasets. Furthermore, the paper does not adequately address the generalizability of MIM4DD's performance gains to datasets with different characteristics, such as higher resolution or more complex features. Although the paper claims the datasets offer a \"balanced perspective\" and that the evaluation is \"exhaustive,\" it doesn't provide a detailed analysis of how these datasets might specifically favor MIM4DD. This lack of discussion on dataset biases and the absence of analysis on performance consistency across dataset characteristics raise concerns about the generalizability of MIM4DD based on the current evidence.",
    "The paper does not thoroughly analyze the computational complexity of MIM4DD or compare it to existing dataset distillation methods. There is no comprehensive theoretical analysis of time and space complexity, including the contrastive learning and mutual information estimation components. The paper lacks explicit quantification of computational costs like training time and memory usage, and it does not compare these costs to other methods on benchmark datasets. The analysis fails to adequately account for the computational overhead introduced by the mutual information maximization component. Furthermore, the paper does not provide a comprehensive discussion of MIM4DD's scalability as dataset size and the number of classes increase. While the paper claims minimal computational overhead and compares MIM4DD to other methods empirically, it lacks a direct comparison of theoretical complexity or quantitative measurements of computational costs across different dataset sizes and numbers of classes.",
    "The paper's ablation studies appear insufficient to fully demonstrate the individual contributions of the MIM4DD framework's components. A major limitation is the absence of Appendix C, which is repeatedly referenced as containing additional ablation details. Without access to Appendix C, a comprehensive evaluation is impossible. Furthermore, the sub-answers reveal a lack of specific quantitative comparisons that isolate the impact of the mutual information maximization component and the contrastive learning framework. For example, there's no explicit analysis of the effect of the contrastive learning framework on the quality and generalization ability of the distilled dataset, nor are there comparisons against dataset distillation techniques that don't utilize contrastive learning. While the paper mentions fixing λ to 0.8 and β to 2.0 in Eq. (17) unless otherwise noted, suggesting some hyperparameter tuning, the extent and rigor of this tuning are unclear without Appendix C. The absence of explicit ablation studies in the main text concerning the sensitivity of MIM4DD to different contrastive loss functions, temperature parameters, embedding dimensionality, or optimization algorithms further weakens the evidence supporting the individual contributions of each component.",
    "The paper's comparison against alternative dataset distillation methods is not sufficiently justified. While the authors provide some rationale for their choices, several weaknesses undermine the comprehensiveness and fairness of the evaluation. First, the paper only partially acknowledges the limitations of MIM4DD, lacking a quantitative comparison of computational cost and a comprehensive discussion of potential failure modes. Second, the justification for using specific benchmark datasets (MNIST, SVHN, CIFAR-10, and CIFAR-100) relies on the claim of 'sufficient' evaluation rather than a comparative discussion of dataset suitability, particularly regarding the exclusion of larger datasets like ImageNet. Third, the paper doesn't provide a detailed justification for excluding methods optimizing for objectives other than mutual information maximization, beyond a general claim about neglecting information theory. Finally, the limited discussion of hyperparameter tuning and the lack of specific hyperparameter values for baseline methods raise concerns about the robustness of the reported performance gains and the fairness of the comparison."
  ]
}