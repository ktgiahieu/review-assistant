
@misc{liuLLMsNarcissisticEvaluators2023,
	title = {{LLMs} as {Narcissistic} {Evaluators}: {When} {Ego} {Inflates} {Evaluation} {Scores}},
	shorttitle = {{LLMs} as {Narcissistic} {Evaluators}},
	url = {https://arxiv.org/abs/2311.09766v1},
	abstract = {Automatic evaluation of generated textual content presents an ongoing challenge within the field of NLP. Given the impressive capabilities of modern language models (LMs) across diverse NLP tasks, there is a growing trend to employ these models in creating innovative evaluation metrics for automated assessment of generation tasks. This paper investigates a pivotal question: Do language model-driven evaluation metrics inherently exhibit bias favoring texts generated by the same underlying language model? Specifically, we assess whether prominent LM-based evaluation metrics--namely, BARTScore, T5Score, and GPTScore--demonstrate a favorable bias toward their respective underlying LMs in the context of summarization tasks. Our findings unveil a latent bias, particularly pronounced when such evaluation metrics are used in an reference-free manner without leveraging gold summaries. These results underscore that assessments provided by generative evaluation models can be influenced by factors beyond the inherent text quality, highlighting the necessity of developing more dependable evaluation protocols in the future.},
	language = {en},
	urldate = {2024-01-11},
	author = {Liu, Yiqi and Moosavi, Nafise Sadat and Lin, Chenghua},
	month = nov,
	year = {2023},
	file = {Liu et al_2023_LLMs as Narcissistic Evaluators.pdf:C\:\\Users\\arjun\\Zotero\\storage\\FB7BE79Y\\Liu et al_2023_LLMs as Narcissistic Evaluators.pdf:application/pdf},
}

@misc{hoelscher-obermaierTuringMirrorEvaluatingAbility2023,
	title = {{TuringMirror}: {Evaluating} the ability of {LLMs} to recognize {LLM}-generated text},
	abstract = {This study investigates the capability of Large Language Models (LLMs) to recognize and distinguish between human-generated and AI-generated text (generated by the LLM under investigation (i.e., itself), or other LLM). Using the TuringMirror benchmark and leveraging the understanding\_fables dataset from BIG-bench, we generated fables using three distinct AI models: gpt-3.5-turbo, gpt-4, and claude-2, and evaluated the stated ability of these LLMs to discern their own and other LLM’s outputs from those generated by other LLMs and humans. Initial findings highlighted the superior performance of gpt-3.5-turbo in several comparison tasks ({\textgreater} 95\% accuracy for recognizing its own text against human text), whereas gpt-4 exhibited notably lower accuracy (way worse than random in two cases). Claude-2's performance remained near the random-guessing threshold. Notably, a consistent positional bias was observed across all models when making predictions, which prompted an error correction to adjust for this bias. The adjusted results provided insights into the true distinguishing capabilities of each model. The study underscores the challenges in effectively distinguishing between AI and human-generated texts using a basic prompting technique and suggests further investigation in refining LLM detection methods and understanding the inherent biases in these models.},
	language = {en},
	author = {Hoelscher-Obermaier, Jason and Lutz, Matthew J. and Feuillade-Montixi and Modak, Sambita},
	month = aug,
	year = {2023},
	note = {Research submission to the Evals research sprint hosted by Apart Research.},
	file = {Hoelscher - Turing Mirror Evaluating the ability of LLMs to r.pdf:C\:\\Users\\arjun\\Zotero\\storage\\7REDN8CQ\\Hoelscher - Turing Mirror Evaluating the ability of LLMs to r.pdf:application/pdf},
}

@misc{pezeshkpourLargeLanguageModels2023,
	title = {Large {Language} {Models} {Sensitivity} to {The} {Order} of {Options} in {Multiple}-{Choice} {Questions}},
	url = {http://arxiv.org/abs/2308.11483},
	doi = {10.48550/arXiv.2308.11483},
	abstract = {Large Language Models (LLMs) have demonstrated remarkable capabilities in various NLP tasks. However, previous works have shown these models are sensitive towards prompt wording, and few-shot demonstrations and their order, posing challenges to fair assessment of these models. As these models become more powerful, it becomes imperative to understand and address these limitations. In this paper, we focus on LLMs robustness on the task of multiple-choice questions -- commonly adopted task to study reasoning and fact-retrieving capability of LLMs. Investigating the sensitivity of LLMs towards the order of options in multiple-choice questions, we demonstrate a considerable performance gap of approximately 13\% to 75\% in LLMs on different benchmarks, when answer options are reordered, even when using demonstrations in a few-shot setting. Through a detailed analysis, we conjecture that this sensitivity arises when LLMs are uncertain about the prediction between the top-2/3 choices, and specific options placements may favor certain prediction between those top choices depending on the question caused by positional bias. We also identify patterns in top-2 choices that amplify or mitigate the model's bias toward option placement. We found that for amplifying bias, the optimal strategy involves positioning the top two choices as the first and last options. Conversely, to mitigate bias, we recommend placing these choices among the adjacent options. To validate our conjecture, we conduct various experiments and adopt two approaches to calibrate LLMs' predictions, leading to up to 8 percentage points improvement across different models and benchmarks.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Pezeshkpour, Pouya and Hruschka, Estevam},
	month = aug,
	year = {2023},
	note = {arXiv:2308.11483 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\DBCIJMFX\\2308.html:text/html;Pezeshkpour_Hruschka_2023_Large Language Models Sensitivity to The Order of Options in Multiple-Choice.pdf:C\:\\Users\\arjun\\Zotero\\storage\\KXCX9F3R\\Pezeshkpour_Hruschka_2023_Large Language Models Sensitivity to The Order of Options in Multiple-Choice.pdf:application/pdf},
}

@misc{kinnimentEvaluatingLanguageModelAgents2024,
	title = {Evaluating {Language}-{Model} {Agents} on {Realistic} {Autonomous} {Tasks}},
	url = {http://arxiv.org/abs/2312.11671},
	doi = {10.48550/arXiv.2312.11671},
	abstract = {In this report, we explore the ability of language model agents to acquire resources, create copies of themselves, and adapt to novel challenges they encounter in the wild. We refer to this cluster of capabilities as "autonomous replication and adaptation" or ARA. We believe that systems capable of ARA could have wide-reaching and hard-to-anticipate consequences, and that measuring and forecasting ARA may be useful for informing measures around security, monitoring, and alignment. Additionally, once a system is capable of ARA, placing bounds on a system's capabilities may become significantly more difficult. We construct four simple example agents that combine language models with tools that allow them to take actions in the world. We then evaluate these agents on 12 tasks relevant to ARA. We find that these language model agents can only complete the easiest tasks from this list, although they make some progress on the more challenging tasks. Unfortunately, these evaluations are not adequate to rule out the possibility that near-future agents will be capable of ARA. In particular, we do not think that these evaluations provide good assurance that the ``next generation'' of language models (e.g. 100x effective compute scaleup on existing models) will not yield agents capable of ARA, unless intermediate evaluations are performed during pretraining. Relatedly, we expect that fine-tuning of the existing models could produce substantially more competent agents, even if the fine-tuning is not directly targeted at ARA.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Kinniment, Megan and Sato, Lucas Jun Koba and Du, Haoxing and Goodrich, Brian and Hasin, Max and Chan, Lawrence and Miles, Luke Harold and Lin, Tao R. and Wijk, Hjalmar and Burget, Joel and Ho, Aaron and Barnes, Elizabeth and Christiano, Paul},
	month = jan,
	year = {2024},
	note = {arXiv:2312.11671 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\LQRUJ5B6\\2312.html:text/html;Kinniment et al_2024_Evaluating Language-Model Agents on Realistic Autonomous Tasks.pdf:C\:\\Users\\arjun\\Zotero\\storage\\X5YTLJUU\\Kinniment et al_2024_Evaluating Language-Model Agents on Realistic Autonomous Tasks.pdf:application/pdf},
}

@misc{hansSpottingLLMsBinoculars2024,
	title = {Spotting {LLMs} {With} {Binoculars}: {Zero}-{Shot} {Detection} of {Machine}-{Generated} {Text}},
	shorttitle = {Spotting {LLMs} {With} {Binoculars}},
	url = {http://arxiv.org/abs/2401.12070},
	doi = {10.48550/arXiv.2401.12070},
	abstract = {Detecting text generated by modern large language models is thought to be hard, as both LLMs and humans can exhibit a wide range of complex behaviors. However, we find that a score based on contrasting two closely related language models is highly accurate at separating human-generated and machine-generated text. Based on this mechanism, we propose a novel LLM detector that only requires simple calculations using a pair of pre-trained LLMs. The method, called Binoculars, achieves state-of-the-art accuracy without any training data. It is capable of spotting machine text from a range of modern LLMs without any model-specific modifications. We comprehensively evaluate Binoculars on a number of text sources and in varied situations. Over a wide range of document types, Binoculars detects over 90\% of generated samples from ChatGPT (and other LLMs) at a false positive rate of 0.01\%, despite not being trained on any ChatGPT data.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Hans, Abhimanyu and Schwarzschild, Avi and Cherepanova, Valeriia and Kazemi, Hamid and Saha, Aniruddha and Goldblum, Micah and Geiping, Jonas and Goldstein, Tom},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12070 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\YGYJW3GL\\2401.html:text/html;Hans et al_2024_Spotting LLMs With Binoculars.pdf:C\:\\Users\\arjun\\Zotero\\storage\\U929HKGS\\Hans et al_2024_Spotting LLMs With Binoculars.pdf:application/pdf},
}

@misc{shenLargeLanguageModels2023,
	title = {Large {Language} {Models} are {Not} {Yet} {Human}-{Level} {Evaluators} for {Abstractive} {Summarization}},
	url = {http://arxiv.org/abs/2305.13091},
	doi = {10.48550/arXiv.2305.13091},
	abstract = {With the recent undeniable advancement in reasoning abilities in large language models (LLMs) like ChatGPT and GPT-4, there is a growing trend for using LLMs on various tasks. One area where LLMs can be employed is as an alternative evaluation metric for complex generative tasks, which generally demands expensive human judges to complement the traditional automatic metrics for various evaluation dimensions such as fluency and consistency. In this work, we conduct extensive analysis to investigate the stability and reliability of LLMs as automatic evaluators for abstractive summarization. We found that while ChatGPT and GPT-4 outperform the commonly used automatic metrics, they are not ready as human replacements due to significant limitations. That is, LLM evaluators rate each candidate system inconsistently and are dimension-dependent. They also struggle to compare candidates with close performance and become more unreliable with higher-quality summaries by obtaining a lower correlation with humans. In other words, with better abstractive summarization systems being introduced at a fast pace, LLMs may result in misleading and unreliable evaluations.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Shen, Chenhui and Cheng, Liying and Nguyen, Xuan-Phi and You, Yang and Bing, Lidong},
	month = oct,
	year = {2023},
	note = {arXiv:2305.13091 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\YBUSMDGM\\2305.html:text/html;Shen et al_2023_Large Language Models are Not Yet Human-Level Evaluators for Abstractive.pdf:C\:\\Users\\arjun\\Zotero\\storage\\7WZ48354\\Shen et al_2023_Large Language Models are Not Yet Human-Level Evaluators for Abstractive.pdf:application/pdf},
}

@misc{hadaAreLargeLanguage2023,
	title = {Are {Large} {Language} {Model}-based {Evaluators} the {Solution} to {Scaling} {Up} {Multilingual} {Evaluation}?},
	url = {http://arxiv.org/abs/2309.07462},
	doi = {10.48550/arXiv.2309.07462},
	abstract = {Large Language Models (LLMs) have demonstrated impressive performance on Natural Language Processing (NLP) tasks, such as Question Answering, Summarization, and Classification. The use of LLMs as evaluators, that can rank or score the output of other models (usually LLMs) has become increasingly popular, due to the limitations of current evaluation techniques including the lack of appropriate benchmarks, metrics, cost, and access to human annotators. While LLMs are capable of handling approximately 100 languages, the majority of languages beyond the top 20 lack systematic evaluation across various tasks, metrics, and benchmarks. This creates an urgent need to scale up multilingual evaluation to ensure a precise understanding of LLM performance across diverse languages. LLM-based evaluators seem like the perfect solution to this problem, as they do not require human annotators, human-created references, or benchmarks and can theoretically be used to evaluate any language covered by the LLM. In this paper, we investigate whether LLM-based evaluators can help scale up multilingual evaluation. Specifically, we calibrate LLM-based evaluation against 20k human judgments of five metrics across three text-generation tasks in eight languages. Our findings indicate that LLM-based evaluators may exhibit bias towards higher scores and should be used with caution and should always be calibrated with a dataset of native speaker judgments, particularly in low-resource and non-Latin script languages.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Hada, Rishav and Gumma, Varun and de Wynter, Adrian and Diddee, Harshita and Ahmed, Mohamed and Choudhury, Monojit and Bali, Kalika and Sitaram, Sunayana},
	month = sep,
	year = {2023},
	note = {arXiv:2309.07462 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\ZVPRZATL\\2309.html:text/html;Hada et al_2023_Are Large Language Model-based Evaluators the Solution to Scaling Up.pdf:C\:\\Users\\arjun\\Zotero\\storage\\LKU337SA\\Hada et al_2023_Are Large Language Model-based Evaluators the Solution to Scaling Up.pdf:application/pdf},
}

@misc{chernCanLargeLanguage2024,
	title = {Can {Large} {Language} {Models} be {Trusted} for {Evaluation}? {Scalable} {Meta}-{Evaluation} of {LLMs} as {Evaluators} via {Agent} {Debate}},
	shorttitle = {Can {Large} {Language} {Models} be {Trusted} for {Evaluation}?},
	url = {http://arxiv.org/abs/2401.16788},
	doi = {10.48550/arXiv.2401.16788},
	abstract = {Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: {\textbackslash}url\{https://github.com/GAIR-NLP/scaleeval\}.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Chern, Steffi and Chern, Ethan and Neubig, Graham and Liu, Pengfei},
	month = jan,
	year = {2024},
	note = {arXiv:2401.16788 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\WL6ILJM9\\2401.html:text/html;Chern et al_2024_Can Large Language Models be Trusted for Evaluation.pdf:C\:\\Users\\arjun\\Zotero\\storage\\LJIS87HN\\Chern et al_2024_Can Large Language Models be Trusted for Evaluation.pdf:application/pdf},
}

@inproceedings{chiangCanLargeLanguage2023,
	address = {Toronto, Canada},
	title = {Can {Large} {Language} {Models} {Be} an {Alternative} to {Human} {Evaluations}?},
	url = {https://aclanthology.org/2023.acl-long.870},
	doi = {10.18653/v1/2023.acl-long.870},
	abstract = {Human evaluation is indispensable and inevitable for assessing the quality of texts generated by machine learning models or written by humans. However, human evaluation is very difficult to reproduce and its quality is notoriously unstable, hindering fair comparisons among different natural language processing (NLP) models and algorithms. Recently, large language models (LLMs) have demonstrated exceptional performance on unseen tasks when only the task instructions are provided. In this paper, we explore if such an ability of the LLMs can be used as an alternative to human evaluation. We present the LLMs with the exact same instructions, samples to be evaluated, and questions used to conduct human evaluation, and then ask the LLMs to generate responses to those questions; we dub this LLM evaluation. We use human evaluation and LLM evaluation to evaluate the texts in two NLP tasks: open-ended story generation and adversarial attacks. We show that the result of LLM evaluation is consistent with the results obtained by expert human evaluation: the texts rated higher by human experts are also rated higher by the LLMs.We also find that the results of LLM evaluation are stable over different formatting of the task instructions and the sampling algorithm used to generate the answer. We are the first to show the potential of using LLMs to assess the quality of texts and discuss the limitations and ethical considerations of LLM evaluation.},
	urldate = {2024-02-05},
	booktitle = {Proceedings of the 61st {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Chiang, Cheng-Han and Lee, Hung-yi},
	editor = {Rogers, Anna and Boyd-Graber, Jordan and Okazaki, Naoaki},
	month = jul,
	year = {2023},
	pages = {15607--15631},
	file = {Chiang_Lee_2023_Can Large Language Models Be an Alternative to Human Evaluations.pdf:C\:\\Users\\arjun\\Zotero\\storage\\AER8MAH7\\Chiang_Lee_2023_Can Large Language Models Be an Alternative to Human Evaluations.pdf:application/pdf},
}

@misc{perezEvaluatingAISystems2023a,
	title = {Towards {Evaluating} {AI} {Systems} for {Moral} {Status} {Using} {Self}-{Reports}},
	url = {http://arxiv.org/abs/2311.08576},
	doi = {10.48550/arXiv.2311.08576},
	abstract = {As AI systems become more advanced and widely deployed, there will likely be increasing debate over whether AI systems could have conscious experiences, desires, or other states of potential moral significance. It is important to inform these discussions with empirical evidence to the extent possible. We argue that under the right circumstances, self-reports, or an AI system's statements about its own internal states, could provide an avenue for investigating whether AI systems have states of moral significance. Self-reports are the main way such states are assessed in humans ("Are you in pain?"), but self-reports from current systems like large language models are spurious for many reasons (e.g. often just reflecting what humans would say). To make self-reports more appropriate for this purpose, we propose to train models to answer many kinds of questions about themselves with known answers, while avoiding or limiting training incentives that bias self-reports. The hope of this approach is that models will develop introspection-like capabilities, and that these capabilities will generalize to questions about states of moral significance. We then propose methods for assessing the extent to which these techniques have succeeded: evaluating self-report consistency across contexts and between similar models, measuring the confidence and resilience of models' self-reports, and using interpretability to corroborate self-reports. We also discuss challenges for our approach, from philosophical difficulties in interpreting self-reports to technical reasons why our proposal might fail. We hope our discussion inspires philosophers and AI researchers to criticize and improve our proposed methodology, as well as to run experiments to test whether self-reports can be made reliable enough to provide information about states of moral significance.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Perez, Ethan and Long, Robert},
	month = nov,
	year = {2023},
	note = {arXiv:2311.08576 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\TGCJ5QTD\\2311.html:text/html;Perez_Long_2023_Towards Evaluating AI Systems for Moral Status Using Self-Reports.pdf:C\:\\Users\\arjun\\Zotero\\storage\\QEUWJMBG\\Perez_Long_2023_Towards Evaluating AI Systems for Moral Status Using Self-Reports.pdf:application/pdf},
}

@misc{wangChatGPTGoodNLG2023,
	title = {Is {ChatGPT} a {Good} {NLG} {Evaluator}? {A} {Preliminary} {Study}},
	shorttitle = {Is {ChatGPT} a {Good} {NLG} {Evaluator}?},
	url = {http://arxiv.org/abs/2303.04048},
	doi = {10.48550/arXiv.2303.04048},
	abstract = {Recently, the emergence of ChatGPT has attracted wide attention from the computational linguistics community. Many prior studies have shown that ChatGPT achieves remarkable performance on various NLP tasks in terms of automatic evaluation metrics. However, the ability of ChatGPT to serve as an evaluation metric is still underexplored. Considering assessing the quality of natural language generation (NLG) models is an arduous task and NLG metrics notoriously show their poor correlation with human judgments, we wonder whether ChatGPT is a good NLG evaluation metric. In this report, we provide a preliminary meta-evaluation on ChatGPT to show its reliability as an NLG metric. In detail, we regard ChatGPT as a human evaluator and give task-specific (e.g., summarization) and aspect-specific (e.g., relevance) instruction to prompt ChatGPT to evaluate the generated results of NLG models. We conduct experiments on five NLG meta-evaluation datasets (including summarization, story generation and data-to-text tasks). Experimental results show that compared with previous automatic metrics, ChatGPT achieves state-of-the-art or competitive correlation with human judgments in most cases. In addition, we find that the effectiveness of the ChatGPT evaluator might be influenced by the creation method of the meta-evaluation datasets. For the meta-evaluation datasets which are created greatly depending on the reference and thus are biased, the ChatGPT evaluator might lose its effectiveness. We hope our preliminary study could prompt the emergence of a general-purposed reliable NLG metric.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Wang, Jiaan and Liang, Yunlong and Meng, Fandong and Sun, Zengkui and Shi, Haoxiang and Li, Zhixu and Xu, Jinan and Qu, Jianfeng and Zhou, Jie},
	month = oct,
	year = {2023},
	note = {arXiv:2303.04048 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\W5VF7FN9\\2303.html:text/html;Wang et al_2023_Is ChatGPT a Good NLG Evaluator.pdf:C\:\\Users\\arjun\\Zotero\\storage\\N8MMNX5V\\Wang et al_2023_Is ChatGPT a Good NLG Evaluator.pdf:application/pdf},
}

@misc{zhengJudgingLLMasaJudgeMTBench2023,
	title = {Judging {LLM}-as-a-{Judge} with {MT}-{Bench} and {Chatbot} {Arena}},
	url = {http://arxiv.org/abs/2306.05685},
	doi = {10.48550/arXiv.2306.05685},
	abstract = {Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm\_judge.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Zheng, Lianmin and Chiang, Wei-Lin and Sheng, Ying and Zhuang, Siyuan and Wu, Zhanghao and Zhuang, Yonghao and Lin, Zi and Li, Zhuohan and Li, Dacheng and Xing, Eric P. and Zhang, Hao and Gonzalez, Joseph E. and Stoica, Ion},
	month = dec,
	year = {2023},
	note = {arXiv:2306.05685 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\N6F82XZX\\2306.html:text/html;Zheng et al_2023_Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf:C\:\\Users\\arjun\\Zotero\\storage\\D8LRWK9T\\Zheng et al_2023_Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena.pdf:application/pdf},
}

@misc{liAlpacaEvalAutomaticEvaluator2024,
	title = {{AlpacaEval}: {An} {Automatic} {Evaluator} of {Instruction}-following {Models}},
	copyright = {Apache-2.0},
	shorttitle = {{AlpacaEval}},
	url = {https://github.com/tatsu-lab/alpaca_eval},
	abstract = {An automatic evaluator for instruction-following language models. Human-validated, high-quality, cheap, and fast.},
	urldate = {2024-02-05},
	author = {Li, Xuechen and Zhang, Tianyi and Dubois, Yann and Taori, Rohan and Gulrajani, Ishaan and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
	month = feb,
	year = {2024},
	note = {original-date: 2023-05-25T09:35:28Z},
}

@misc{fuGPTScoreEvaluateYou2023,
	title = {{GPTScore}: {Evaluate} as {You} {Desire}},
	shorttitle = {{GPTScore}},
	url = {http://arxiv.org/abs/2302.04166},
	doi = {10.48550/arXiv.2302.04166},
	abstract = {Generative Artificial Intelligence (AI) has enabled the development of sophisticated models that are capable of producing high-caliber text, images, and other outputs through the utilization of large pre-trained models. Nevertheless, assessing the quality of the generation is an even more arduous task than the generation itself, and this issue has not been given adequate consideration recently. This paper proposes a novel evaluation framework, GPTScore, which utilizes the emergent abilities (e.g., zero-shot instruction) of generative pre-trained models to score generated texts. There are 19 pre-trained models explored in this paper, ranging in size from 80M (e.g., FLAN-T5-small) to 175B (e.g., GPT3). Experimental results on four text generation tasks, 22 evaluation aspects, and corresponding 37 datasets demonstrate that this approach can effectively allow us to achieve what one desires to evaluate for texts simply by natural language instructions. This nature helps us overcome several long-standing challenges in text evaluation--how to achieve customized, multi-faceted evaluation without the need for annotated samples. We make our code publicly available at https://github.com/jinlanfu/GPTScore.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Fu, Jinlan and Ng, See-Kiong and Jiang, Zhengbao and Liu, Pengfei},
	month = feb,
	year = {2023},
	note = {arXiv:2302.04166 [cs]},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\PSVCL3BG\\2302.html:text/html;Fu et al_2023_GPTScore.pdf:C\:\\Users\\arjun\\Zotero\\storage\\AIBPZCHX\\Fu et al_2023_GPTScore.pdf:application/pdf},
}

@misc{kooBenchmarkingCognitiveBiases2023,
	title = {Benchmarking {Cognitive} {Biases} in {Large} {Language} {Models} as {Evaluators}},
	url = {http://arxiv.org/abs/2309.17012},
	doi = {10.48550/arXiv.2309.17012},
	abstract = {Large Language Models (LLMs) have recently been shown to be effective as automatic evaluators with simple prompting and in-context learning. In this work, we assemble 15 LLMs of four different size ranges and evaluate their output responses by preference ranking from the other LLMs as evaluators, such as System Star is better than System Square. We then evaluate the quality of ranking outputs introducing the Cognitive Bias Benchmark for LLMs as Evaluators (CoBBLEr), a benchmark to measure six different cognitive biases in LLM evaluation outputs, such as the Egocentric bias where a model prefers to rank its own outputs highly in evaluation. We find that LLMs are biased text quality evaluators, exhibiting strong indications on our bias benchmark (average of 40\% of comparisons across all models) within each of their evaluations that question their robustness as evaluators. Furthermore, we examine the correlation between human and machine preferences and calculate the average Rank-Biased Overlap (RBO) score to be 49.6\%, indicating that machine preferences are misaligned with humans. According to our findings, LLMs may still be unable to be utilized for automatic annotation aligned with human preferences. Our project page is at: https://minnesotanlp.github.io/cobbler.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Koo, Ryan and Lee, Minhwa and Raheja, Vipul and Park, Jong Inn and Kim, Zae Myung and Kang, Dongyeop},
	month = sep,
	year = {2023},
	note = {arXiv:2309.17012 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\2EH87R3Y\\2309.html:text/html;Koo et al_2023_Benchmarking Cognitive Biases in Large Language Models as Evaluators.pdf:C\:\\Users\\arjun\\Zotero\\storage\\RYDZPPNS\\Koo et al_2023_Benchmarking Cognitive Biases in Large Language Models as Evaluators.pdf:application/pdf},
}

@misc{bittonVisITBenchBenchmarkVisionLanguage2023a,
	title = {{VisIT}-{Bench}: {A} {Benchmark} for {Vision}-{Language} {Instruction} {Following} {Inspired} by {Real}-{World} {Use}},
	shorttitle = {{VisIT}-{Bench}},
	url = {http://arxiv.org/abs/2308.06595},
	doi = {10.48550/arXiv.2308.06595},
	abstract = {We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluation of instruction-following vision-language models for real-world use. Our starting point is curating 70 'instruction families' that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27\% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at visit-bench.github.io.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Bitton, Yonatan and Bansal, Hritik and Hessel, Jack and Shao, Rulin and Zhu, Wanrong and Awadalla, Anas and Gardner, Josh and Taori, Rohan and Schmidt, Ludwig},
	month = dec,
	year = {2023},
	note = {arXiv:2308.06595 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\YYQDX7UK\\2308.html:text/html;Bitton et al_2023_VisIT-Bench.pdf:C\:\\Users\\arjun\\Zotero\\storage\\WTGBUNG6\\Bitton et al_2023_VisIT-Bench.pdf:application/pdf},
}

@misc{shashidharDemocratizingLLMsExploration2023,
	title = {Democratizing {LLMs}: {An} {Exploration} of {Cost}-{Performance} {Trade}-offs in {Self}-{Refined} {Open}-{Source} {Models}},
	shorttitle = {Democratizing {LLMs}},
	url = {http://arxiv.org/abs/2310.07611},
	doi = {10.48550/arXiv.2310.07611},
	abstract = {The dominance of proprietary LLMs has led to restricted access and raised information privacy concerns. High-performing open-source alternatives are crucial for information-sensitive and high-volume applications but often lag behind in performance. To address this gap, we propose (1) A untargeted variant of iterative self-critique and self-refinement devoid of external influence. (2) A novel ranking metric - Performance, Refinement, and Inference Cost Score (PeRFICS) - to find the optimal model for a given task considering refined performance and cost. Our experiments show that SoTA open source models of varying sizes from 7B - 65B, on average, improve 8.2\% from their baseline performance. Strikingly, even models with extremely small memory footprints, such as Vicuna-7B, show a 11.74\% improvement overall and up to a 25.39\% improvement in high-creativity, open ended tasks on the Vicuna benchmark. Vicuna-13B takes it a step further and outperforms ChatGPT post-refinement. This work has profound implications for resource-constrained and information-sensitive environments seeking to leverage LLMs without incurring prohibitive costs, compromising on performance and privacy. The domain-agnostic self-refinement process coupled with our novel ranking metric facilitates informed decision-making in model selection, thereby reducing costs and democratizing access to high-performing language models, as evidenced by case studies.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Shashidhar, Sumuk and Chinta, Abhinav and Sahai, Vaibhav and Wang, Zhenhailong and Ji, Heng},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07611 [cs]},
	keywords = {68T50 (Primary), A.2, C.4, Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Performance, H.3.4, I.2.7, K.4.1},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\4PKV27S6\\2310.html:text/html;Shashidhar et al_2023_Democratizing LLMs.pdf:C\:\\Users\\arjun\\Zotero\\storage\\LU4RKLBT\\Shashidhar et al_2023_Democratizing LLMs.pdf:application/pdf},
}

@misc{zengEvaluatingLargeLanguage2023,
	title = {Evaluating {Large} {Language} {Models} at {Evaluating} {Instruction} {Following}},
	url = {http://arxiv.org/abs/2310.07641},
	abstract = {As research in large language models (LLMs) continues to accelerate, LLM-based evaluation has emerged as a scalable and cost-effective alternative to human evaluations for comparing the ever increasing list of models. This paper investigates the efficacy of these "LLM evaluators", particularly in using them to assess instruction following, a metric that gauges how closely generated text adheres to the given instruction. We introduce a challenging meta-evaluation benchmark, LLMBar, designed to test the ability of an LLM evaluator in discerning instruction-following outputs. The authors manually curated 419 pairs of outputs, one adhering to instructions while the other diverging, yet may possess deceptive qualities that mislead an LLM evaluator, e.g., a more engaging tone. Contrary to existing meta-evaluation, we discover that different evaluators (i.e., combinations of LLMs and prompts) exhibit distinct performance on LLMBar and even the highest-scoring ones have substantial room for improvement. We also present a novel suite of prompting strategies that further close the gap between LLM and human evaluators. With LLMBar, we hope to offer more insight into LLM evaluators and foster future research in developing better instruction-following models.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Zeng, Zhiyuan and Yu, Jiatong and Gao, Tianyu and Meng, Yu and Goyal, Tanya and Chen, Danqi},
	month = oct,
	year = {2023},
	note = {arXiv:2310.07641 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\HIH8US2H\\2310.html:text/html;Zeng et al_2023_Evaluating Large Language Models at Evaluating Instruction Following.pdf:C\:\\Users\\arjun\\Zotero\\storage\\24DFXRGA\\Zeng et al_2023_Evaluating Large Language Models at Evaluating Instruction Following.pdf:application/pdf},
}

@misc{yuanEvaluatingInstructionTunedLarge2023,
	title = {Evaluating {Instruction}-{Tuned} {Large} {Language} {Models} on {Code} {Comprehension} and {Generation}},
	url = {http://arxiv.org/abs/2308.01240},
	abstract = {In this work, we evaluate 10 open-source instructed LLMs on four representative code comprehension and generation tasks. We have the following main findings. First, for the zero-shot setting, instructed LLMs are very competitive on code comprehension and generation tasks and sometimes even better than small SOTA models specifically fine-tuned on each downstream task. We also find that larger instructed LLMs are not always better on code-related tasks. Second, for the few-shot setting, we find that adding demonstration examples substantially helps instructed LLMs perform better on most code comprehension and generation tasks; however, the examples would sometimes induce unstable or even worse performance. Furthermore, we find widely-used BM25-based shot selection strategy significantly outperforms the basic random selection or fixed selection only on generation problems. Third, for the fine-tuning setting, we find that fine-tuning could further improve the model performance on downstream code comprehension and generation tasks compared to the zero-shot/one-shot performance. In addition, after being fine-tuned on the same downstream task dataset, instructed LLMs outperform both the small SOTA models and similar-scaled LLMs without instruction tuning. Based on our findings, we further present practical implications on model and usage recommendation, performance and cost trade-offs, and future direction.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Yuan, Zhiqiang and Liu, Junwei and Zi, Qiancheng and Liu, Mingwei and Peng, Xin and Lou, Yiling},
	month = aug,
	year = {2023},
	note = {arXiv:2308.01240 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\NGI5DRM7\\2308.html:text/html;Yuan et al_2023_Evaluating Instruction-Tuned Large Language Models on Code Comprehension and.pdf:C\:\\Users\\arjun\\Zotero\\storage\\SHEZLUYQ\\Yuan et al_2023_Evaluating Instruction-Tuned Large Language Models on Code Comprehension and.pdf:application/pdf},
}

@misc{perezDiscoveringLanguageModel2022,
	title = {Discovering {Language} {Model} {Behaviors} with {Model}-{Written} {Evaluations}},
	url = {http://arxiv.org/abs/2212.09251},
	doi = {10.48550/arXiv.2212.09251},
	abstract = {As language models (LMs) scale, they develop many novel behaviors, good and bad, exacerbating the need to evaluate how they behave. Prior work creates evaluations with crowdwork (which is time-consuming and expensive) or existing data sources (which are not always available). Here, we automatically generate evaluations with LMs. We explore approaches with varying amounts of human effort, from instructing LMs to write yes/no questions to making complex Winogender schemas with multiple stages of LM-based generation and filtering. Crowdworkers rate the examples as highly relevant and agree with 90-100\% of labels, sometimes more so than corresponding human-written datasets. We generate 154 datasets and discover new cases of inverse scaling where LMs get worse with size. Larger LMs repeat back a dialog user's preferred answer ("sycophancy") and express greater desire to pursue concerning goals like resource acquisition and goal preservation. We also find some of the first examples of inverse scaling in RL from Human Feedback (RLHF), where more RLHF makes LMs worse. For example, RLHF makes LMs express stronger political views (on gun rights and immigration) and a greater desire to avoid shut down. Overall, LM-written evaluations are high-quality and let us quickly discover many novel LM behaviors.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Perez, Ethan and Ringer, Sam and Lukošiūtė, Kamilė and Nguyen, Karina and Chen, Edwin and Heiner, Scott and Pettit, Craig and Olsson, Catherine and Kundu, Sandipan and Kadavath, Saurav and Jones, Andy and Chen, Anna and Mann, Ben and Israel, Brian and Seethor, Bryan and McKinnon, Cameron and Olah, Christopher and Yan, Da and Amodei, Daniela and Amodei, Dario and Drain, Dawn and Li, Dustin and Tran-Johnson, Eli and Khundadze, Guro and Kernion, Jackson and Landis, James and Kerr, Jamie and Mueller, Jared and Hyun, Jeeyoon and Landau, Joshua and Ndousse, Kamal and Goldberg, Landon and Lovitt, Liane and Lucas, Martin and Sellitto, Michael and Zhang, Miranda and Kingsland, Neerav and Elhage, Nelson and Joseph, Nicholas and Mercado, Noemí and DasSarma, Nova and Rausch, Oliver and Larson, Robin and McCandlish, Sam and Johnston, Scott and Kravec, Shauna and Showk, Sheer El and Lanham, Tamera and Telleen-Lawton, Timothy and Brown, Tom and Henighan, Tom and Hume, Tristan and Bai, Yuntao and Hatfield-Dodds, Zac and Clark, Jack and Bowman, Samuel R. and Askell, Amanda and Grosse, Roger and Hernandez, Danny and Ganguli, Deep and Hubinger, Evan and Schiefer, Nicholas and Kaplan, Jared},
	month = dec,
	year = {2022},
	note = {arXiv:2212.09251 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\5ZY6ZD3A\\2212.html:text/html;Perez et al_2022_Discovering Language Model Behaviors with Model-Written Evaluations.pdf:C\:\\Users\\arjun\\Zotero\\storage\\AK5YY57I\\Perez et al_2022_Discovering Language Model Behaviors with Model-Written Evaluations.pdf:application/pdf},
}

@misc{madaanSelfRefineIterativeRefinement2023,
	title = {Self-{Refine}: {Iterative} {Refinement} with {Self}-{Feedback}},
	shorttitle = {Self-{Refine}},
	url = {http://arxiv.org/abs/2303.17651},
	doi = {10.48550/arXiv.2303.17651},
	abstract = {Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by {\textasciitilde}20\% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.},
	urldate = {2024-02-05},
	publisher = {arXiv},
	author = {Madaan, Aman and Tandon, Niket and Gupta, Prakhar and Hallinan, Skyler and Gao, Luyu and Wiegreffe, Sarah and Alon, Uri and Dziri, Nouha and Prabhumoye, Shrimai and Yang, Yiming and Gupta, Shashank and Majumder, Bodhisattwa Prasad and Hermann, Katherine and Welleck, Sean and Yazdanbakhsh, Amir and Clark, Peter},
	month = may,
	year = {2023},
	note = {arXiv:2303.17651 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\RDKVLI8U\\2303.html:text/html;Madaan et al_2023_Self-Refine.pdf:C\:\\Users\\arjun\\Zotero\\storage\\3UN5B574\\Madaan et al_2023_Self-Refine.pdf:application/pdf},
}

@misc{srivastavaImitationGameQuantifying2023,
	title = {Beyond the {Imitation} {Game}: {Quantifying} and extrapolating the capabilities of language models},
	shorttitle = {Beyond the {Imitation} {Game}},
	url = {http://arxiv.org/abs/2206.04615},
	doi = {10.48550/arXiv.2206.04615},
	abstract = {Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit "breakthrough" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Srivastava, Aarohi and Rastogi, Abhinav and Rao, Abhishek et al.},
	month = jun,
	year = {2023},
	note = {arXiv:2206.04615 [cs, stat]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Computers and Society, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\U8XIZ623\\2206.html:text/html;Srivastava et al_2023_Beyond the Imitation Game.pdf:C\:\\Users\\arjun\\Zotero\\storage\\NMZ5L6NK\\Srivastava et al_2023_Beyond the Imitation Game.pdf:application/pdf},
}

@misc{mukobiWelfareDiplomacyBenchmarking2023,
	title = {Welfare {Diplomacy}: {Benchmarking} {Language} {Model} {Cooperation}},
	shorttitle = {Welfare {Diplomacy}},
	url = {http://arxiv.org/abs/2310.08901},
	abstract = {The growing capabilities and increasingly widespread deployment of AI systems necessitate robust benchmarks for measuring their cooperative capabilities. Unfortunately, most multi-agent benchmarks are either zero-sum or purely cooperative, providing limited opportunities for such measurements. We introduce a general-sum variant of the zero-sum board game Diplomacy -- called Welfare Diplomacy -- in which players must balance investing in military conquest and domestic welfare. We argue that Welfare Diplomacy facilitates both a clearer assessment of and stronger training incentives for cooperative capabilities. Our contributions are: (1) proposing the Welfare Diplomacy rules and implementing them via an open-source Diplomacy engine; (2) constructing baseline agents using zero-shot prompted language models; and (3) conducting experiments where we find that baselines using state-of-the-art models attain high social welfare but are exploitable. Our work aims to promote societal safety by aiding researchers in developing and assessing multi-agent AI systems. Code to evaluate Welfare Diplomacy and reproduce our experiments is available at https://github.com/mukobi/welfare-diplomacy.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Mukobi, Gabriel and Erlebach, Hannah and Lauffer, Niklas and Hammond, Lewis and Chan, Alan and Clifton, Jesse},
	month = oct,
	year = {2023},
	note = {arXiv:2310.08901 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Multiagent Systems},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\IQAA2P4V\\2310.html:text/html;Mukobi et al_2023_Welfare Diplomacy.pdf:C\:\\Users\\arjun\\Zotero\\storage\\SVQYR2LG\\Mukobi et al_2023_Welfare Diplomacy.pdf:application/pdf},
}

@misc{dafoeOpenProblemsCooperative2020,
	title = {Open {Problems} in {Cooperative} {AI}},
	url = {http://arxiv.org/abs/2012.08630},
	doi = {10.48550/arXiv.2012.08630},
	abstract = {Problems of cooperation--in which agents seek ways to jointly improve their welfare--are ubiquitous and important. They can be found at scales ranging from our daily routines--such as driving on highways, scheduling meetings, and working collaboratively--to our global challenges--such as peace, commerce, and pandemic preparedness. Arguably, the success of the human species is rooted in our ability to cooperate. Since machines powered by artificial intelligence are playing an ever greater role in our lives, it will be important to equip them with the capabilities necessary to cooperate and to foster cooperation. We see an opportunity for the field of artificial intelligence to explicitly focus effort on this class of problems, which we term Cooperative AI. The objective of this research would be to study the many aspects of the problems of cooperation and to innovate in AI to contribute to solving these problems. Central goals include building machine agents with the capabilities needed for cooperation, building tools to foster cooperation in populations of (machine and/or human) agents, and otherwise conducting AI research for insight relevant to problems of cooperation. This research integrates ongoing work on multi-agent systems, game theory and social choice, human-machine interaction and alignment, natural-language processing, and the construction of social tools and platforms. However, Cooperative AI is not the union of these existing areas, but rather an independent bet about the productivity of specific kinds of conversations that involve these and other areas. We see opportunity to more explicitly focus on the problem of cooperation, to construct unified theory and vocabulary, and to build bridges with adjacent communities working on cooperation, including in the natural, social, and behavioural sciences.},
	urldate = {2024-02-06},
	publisher = {arXiv},
	author = {Dafoe, Allan and Hughes, Edward and Bachrach, Yoram and Collins, Tantum and McKee, Kevin R. and Leibo, Joel Z. and Larson, Kate and Graepel, Thore},
	month = dec,
	year = {2020},
	note = {arXiv:2012.08630 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Multiagent Systems},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\UKP58P6Z\\2012.html:text/html;Dafoe et al_2020_Open Problems in Cooperative AI.pdf:C\:\\Users\\arjun\\Zotero\\storage\\XH3VHHCQ\\Dafoe et al_2020_Open Problems in Cooperative AI.pdf:application/pdf},
}

@inproceedings{nallapatiAbstractiveTextSummarization2016,
	address = {Berlin, Germany},
	title = {Abstractive {Text} {Summarization} using {Sequence}-to-sequence {RNNs} and {Beyond}},
	url = {https://aclanthology.org/K16-1028},
	doi = {10.18653/v1/K16-1028},
	urldate = {2024-02-07},
	booktitle = {Proceedings of the 20th {SIGNLL} {Conference} on {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Nallapati, Ramesh and Zhou, Bowen and dos Santos, Cicero and Gulcehre, Caglar and Xiang, Bing},
	editor = {Riezler, Stefan and Goldberg, Yoav},
	month = aug,
	year = {2016},
	pages = {280--290},
	file = {Nallapati et al_2016_Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.pdf:C\:\\Users\\arjun\\Zotero\\storage\\X7STYCBU\\Nallapati et al_2016_Abstractive Text Summarization using Sequence-to-sequence RNNs and Beyond.pdf:application/pdf},
}

@misc{narayanDonGiveMe2018a,
	title = {Don't {Give} {Me} the {Details}, {Just} the {Summary}! {Topic}-{Aware} {Convolutional} {Neural} {Networks} for {Extreme} {Summarization}},
	url = {http://arxiv.org/abs/1808.08745},
	abstract = {We introduce extreme summarization, a new single-document summarization task which does not favor extractive strategies and calls for an abstractive modeling approach. The idea is to create a short, one-sentence news summary answering the question "What is the article about?". We collect a real-world, large-scale dataset for this task by harvesting online articles from the British Broadcasting Corporation (BBC). We propose a novel abstractive model which is conditioned on the article's topics and based entirely on convolutional neural networks. We demonstrate experimentally that this architecture captures long-range dependencies in a document and recognizes pertinent content, outperforming an oracle extractive system and state-of-the-art abstractive approaches when evaluated automatically and by humans.},
	urldate = {2024-02-07},
	publisher = {arXiv},
	author = {Narayan, Shashi and Cohen, Shay B. and Lapata, Mirella},
	month = aug,
	year = {2018},
	note = {arXiv:1808.08745 [cs]
version: 1},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\UUAWWLTM\\1808.html:text/html;Narayan et al_2018_Don't Give Me the Details, Just the Summary.pdf:C\:\\Users\\arjun\\Zotero\\storage\\23XFDHDD\\Narayan et al_2018_Don't Give Me the Details, Just the Summary.pdf:application/pdf},
}


@article{hacklGPT4ReliableRater2023,
	title = {Is {GPT}-4 a reliable rater? {Evaluating} {Consistency} in {GPT}-4 {Text} {Ratings}},
	volume = {8},
	issn = {2504-284X},
	shorttitle = {Is {GPT}-4 a reliable rater?},
	url = {http://arxiv.org/abs/2308.02575},
	doi = {10.3389/feduc.2023.1272229},
	abstract = {This study investigates the consistency of feedback ratings generated by OpenAI's GPT-4, a state-of-the-art artificial intelligence language model, across multiple iterations, time spans and stylistic variations. The model rated responses to tasks within the Higher Education (HE) subject domain of macroeconomics in terms of their content and style. Statistical analysis was conducted in order to learn more about the interrater reliability, consistency of the ratings across iterations and the correlation between ratings in terms of content and style. The results revealed a high interrater reliability with ICC scores ranging between 0.94 and 0.99 for different timespans, suggesting that GPT-4 is capable of generating consistent ratings across repetitions with a clear prompt. Style and content ratings show a high correlation of 0.87. When applying a non-adequate style the average content ratings remained constant, while style ratings decreased, which indicates that the large language model (LLM) effectively distinguishes between these two criteria during evaluation. The prompt used in this study is furthermore presented and explained. Further research is necessary to assess the robustness and reliability of AI models in various use cases.},
	urldate = {2024-04-10},
	journal = {Frontiers in Education},
	author = {Hackl, Veronika and Müller, Alexandra Elena and Granitzer, Michael and Sailer, Maximilian},
	month = dec,
	year = {2023},
	note = {arXiv:2308.02575 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	pages = {1272229},
	file = {arXiv.org Snapshot:C\:\\Users\\arjun\\Zotero\\storage\\96LP4Q9V\\2308.html:text/html;Hackl et al_2023_Is GPT-4 a reliable rater.pdf:C\:\\Users\\arjun\\Zotero\\storage\\AG9IHANZ\\Hackl et al_2023_Is GPT-4 a reliable rater.pdf:application/pdf},
}

